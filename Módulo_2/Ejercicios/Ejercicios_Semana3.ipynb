{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8ZNXg4XORU1b7XHnnOBQq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Ejercicio 1: Reducción de dimensionalidad ##\n","\n","Es este ejercicio vamos a trabajar con los datasets que ya usamos anteiormente de medidas de personas del ejército de EEUU `ansurMen.csv` y `ansurMen.csv`."],"metadata":{"id":"prGVkzsBqmVA"}},{"cell_type":"markdown","source":["**a)** Carguen los dos csvs en dos dataframes distintos de pandas. Agréguenle a cada uno una nueva columna 'SEXO' que tenga los valores 'H' y 'M', según corresponda, para poder identificar de qué dataset vino cada persona. Luego unan los dos datasets en uno nuevo usando la función de pandas `pd.concat([df1, df2])`."],"metadata":{"id":"V_O_4bVAODbL"}},{"cell_type":"code","source":[],"metadata":{"id":"fSsNrHZTVsiB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**b)** Definan un nuevo dataframe de variables sólo numéricas a partir del anterior, descartando las columnas 'SEXO' y 'SUBJECT_NUMBER' (¿tiene sentido quedarse con esta última columna?). Luego apliquenle el `StandardScaler` de `sklearn` a este nuevo dataframe, y hagan una reducción dimensional usando PCA. ¿Con cuántas componentes necesito quedarme para explicar el 95% de la varianza de los datos?"],"metadata":{"id":"-urojOMXPTzy"}},{"cell_type":"code","source":[],"metadata":{"id":"bgKon5-uVszn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**c)** Ahora hagan otro PCA, pero quedándose sólo con 2 componentes, y hagan un scatterplot de los datos. ¿Qué es lo que se ve? Traten de pintar los puntos usando la columna categórica \"SEXO\" que tiene el dataset original.  "],"metadata":{"id":"T-wpFD4IRozX"}},{"cell_type":"code","source":[],"metadata":{"id":"UHz2h6GQVtQu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**d) (Opcional)**. Ahora hagan un PCA con un número reducido de componentes (digamos 8), y luego apliquen un TSNE con 2 componentes. Grafiquen los resultados cómo hicieron en el punto anterior. ¿Qué se ve ahora? Pueden jugar con el número de componentes del PCA, o sólo hacer TSNE, y ver las diferencias.  "],"metadata":{"id":"3jvcJy5uSZT0"}},{"cell_type":"code","source":[],"metadata":{"id":"93-VXnEzVunY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ejercicio 2: Preprocesamiento ##\n","\n","En este ejercico vamos a trabajar con un dataset bastante problemático: el dataset de arbolado en calles de CABA, [arbolado-publico-lineal-2017-2018.csv](https://drive.google.com/file/d/1tCbEg1Yy0xgmY5e3hMgbOGYEiJ9Ffw8P/view?usp=sharing) (extraído de [aquí](https://data.buenosaires.gob.ar/dataset/arbolado-publico-lineal)). El mismo el similar al de árboles en parques que ya hemos usado, pero tiene bastantes más registros."],"metadata":{"id":"JDrgJTXgq-Es"}},{"cell_type":"markdown","source":["**a) Cargando los datos**. Importen este nuevo dataset usando pandas. Van a notar que les da una advertencia (*warning*) porque hay algunas columnas con tipos mezclados. Por ahora ignorenlo. \n","\n","Para ahorrarnos trabajo, definan un nuevo DataFrame usando solo las columnas `['nro_registro', 'nombre_cientifico', 'estado_plantera', 'ubicacion_plantera', 'nivel_plantera', 'diametro_altura_pecho', 'altura_arbol']`."],"metadata":{"id":"d65eq26a8SJw"}},{"cell_type":"code","source":[],"metadata":{"id":"caE1pg5vVwIB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**b) Limpieza de datos (I)**. Analicen los valores únicos que pueden tomar las columnas 'estado_plantera', 'ubicacion_plantera' y 'nivel_plantera'. ¿Qué es lo que ven?\n","\n","Para las tres columnas, unifiquen los valores que pertecen a una misma catgoría."],"metadata":{"id":"Ps1qPkfN_5EO"}},{"cell_type":"code","source":[],"metadata":{"id":"FI-gVKVBVxWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**c) Limpieza de datos (II)**. Hagan histogramas de los valores de las variables 'diametro_altura_pecho' y 'altura_arbol'.\n","\n","A primera vista no parece haber nada raro, pero fijense que para el diámetro (que está medido en cm) hay muchos datos con valor 0 (pueden usar el método `value_counts()`). Si bien podría haber árboles con menos de 1 cm de diámetro, la cantidad de los mismos nos hace sospechar que en gran parte de los casos se trata de un error.\n","\n","Eliminen las filas con diámetro 0, o al menos por ahora reemplacen el valor por `nan`."],"metadata":{"id":"JWd5xkxlBPDN"}},{"cell_type":"code","source":[],"metadata":{"id":"MvZ95ElIVymZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**d) Datos faltantes**. Analicen la cantidad de datos faltantes en cada columna y decidan qué hacer con ellos (descartarlos, crear una nueva categoría en las variables categóricas, reemplazarla por promedio/mediana en las numéricas, etc.)"],"metadata":{"id":"y2cmEq6qHvN8"}},{"cell_type":"code","source":[],"metadata":{"id":"6VBFMz4rVz5i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**e) Variables categóricas**. Apliquen el método de One-Hot Encoding a alguna de las variables categóricas del dataset. ¿De qué va a depender la cantidad de componentes de los vectores resultantes?"],"metadata":{"id":"_ZBJj9AxMxVV"}},{"cell_type":"code","source":[],"metadata":{"id":"RCdSsUroMx46"},"execution_count":null,"outputs":[]}]}