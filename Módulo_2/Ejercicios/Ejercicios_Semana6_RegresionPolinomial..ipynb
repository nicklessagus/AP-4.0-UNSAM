{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Argentina programa 4.0 - M贸dulo 2: Ciencia de Datos\n",
        "\n",
        "---\n",
        "## Ejerc铆cios Semana 6. Regresion Polinomial\n"
      ],
      "metadata": {
        "id": "M5MOMyK2PAFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresi贸n polinomial con datos reales: datos Properati"
      ],
      "metadata": {
        "id": "XaYhhUoU7qh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Volvamos al conjunto de datos Properati, que hemos usado en la semana anterior.\n",
        "\n",
        "Despues de filtrar bien los datos, hicimos un modelo lienal para predecir el valor de las propiedades basados en su superf铆cie:\n",
        "\n",
        "precio [USD]=0+1sup. total.\n",
        "\n",
        "* 驴Qu茅 pasa ahora si usamos un modelo polinomial?\n",
        "* 驴Podemos chequear si un modelo m谩s complejo describir谩 mejor esos datos?\n",
        "* 驴Hasta que orden del polin贸mio tiene sentido ir?\n",
        "\n"
      ],
      "metadata": {
        "id": "REhD-yzK7wRj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie2zV3qOdUiC"
      },
      "source": [
        "## Otra forma de regularizar: Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAOhuZsjdUiC"
      },
      "source": [
        "Durante la clase, vimos una forma de regularizaci贸n, conocida por _ridge_ en la que se le agrega a la funci贸n error, la suma de los cuadrados de todos los coeficientes, con um peso $\\lambda$/2:\n",
        "$$\n",
        "E_\\text{ridge}(\\boldsymbol{\\omega}; \\lambda) = \\frac{1}{2} \\sum_{i=1}^{N} \\left\\{y(x_i, \\boldsymbol{\\omega}) - t_i\\right\\}^2\n",
        " + \\frac{\\lambda}{2} \\sum_{i=1}^M \\omega_i^2\\;\\;.\n",
        "$$\n",
        "El nuevo t茅rmino es el t茅rmino de regularizaci贸n (o penalizaci贸n).\n",
        "\n",
        "Como la suma de los cuadrados de todos los coeficientes corresponde al cuadrado de la _norma_ (el m贸dulo) del vector de coeficientes, se dice que esa regularizaci贸n emplea la _norma_ $l2$.\n",
        "\n",
        "Otra regresi贸n regularizada que se utiliza a menudo es la regresi贸n **LASSO (_least absolute shrinkage and selection operator_ / operador de reducci贸n y selecci贸n m铆nima absoluta)**, que selecciona de forma natural las variables m谩s relevantes y produce modelos m谩s parsimoniosos.\n",
        "\n",
        "En lugar de penalizar la funci贸n de error utilizando la suma de los cuadrados de los par谩metros del modelo, como en el caso anterior, **LASSO** explota la norma $l1$, que es simplemente la suma de los *valores absolutos* de los par谩metros del modelo.\n",
        "\n",
        "En otras palabras, la norma $l1$ de un vector es, simplemente:\n",
        "\n",
        "$$\n",
        "||\\boldsymbol{\\omega}||_1 = \\sum_i |\\omega_i|\\;\\;.\n",
        "$$\n",
        "\n",
        "La funci贸n de error modificada es, por lo tanto,\n",
        "$$\n",
        "E_\\text{lasso}(\\boldsymbol{\\omega}; \\lambda) = \\frac{1}{2} \\sum_{i=1}^{N} \\left\\{y(x_i, \\boldsymbol{\\omega}) - t_i\\right\\}^2\n",
        " + \\frac{\\lambda}{2} \\sum_{i=1}^M |\\omega_i|\\;\\;,\n",
        "$$\n",
        "donde nuevamente introducimos el hiperpar谩metro $\\lambda$ para controlar el nivel de penalizaci贸n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0KPfXhAdUiC"
      },
      "source": [
        "###Adi贸s soluciones anal铆ticas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz90zzHNdUiD"
      },
      "source": [
        "La primera consecuencia de esta elecci贸n de la penalizaci贸n es que la funci贸n de error ya no puede optimizarse (minimizarse) anal铆ticamente. Es necesario recurrir, entonces, a diferentes algoritmos iterativos.\n",
        "\n",
        "En `sklearn`, hay dos implementaciones:\n",
        "\n",
        "* `linear_model.Lasso` usa *descenso por coordenadas* para encontrar el m铆nimo de la funci贸n de error.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Coordinate_descent.svg/500px-Coordinate_descent.svg.png\" width=500px></img>\n",
        "\n",
        "* `linear_model.LassoLars` utiliza LARS (regresi贸n de 谩ngulo m铆nimo / _least angle regression_), estrechamente relacionado con _forward stepwise regression_ (es decir, todos los coeficientes comienzan en cero, $\\boldsymbol{\\omega} = 0$, y se incrementan progresivamente). Pueden leer m谩s en la [documentaci贸n de scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEjg2nn8dUiD"
      },
      "source": [
        "### Implementaci贸n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7haoQJVGdUiD"
      },
      "source": [
        "Importen `Lasso` y `LassoLars` y exploren su documentaci贸n y argumentos. 驴Cu谩les son los par谩metros que est谩n asociados al procedimiento de optimizaci贸n?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spNPt770dUiD"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso, LassoLars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etlrJXmFdUiE"
      },
      "source": [
        "Creen una funci贸n `lasso` tal y como hicimos para la regresi贸n de _ridge_. Elijan una implementaci贸n de LASSO. Pueden ver los par谩metros de Lasso utilizando `Lasso?`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5jTnK4udUiE"
      },
      "outputs": [],
      "source": [
        "Lasso?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFNCNCQkdUiE"
      },
      "outputs": [],
      "source": [
        "def lasso(m, ll):\n",
        "    return Pipeline([('poly_features', PolynomialFeatures(degree=m)),\n",
        "                     ('regressor', Lasso(alpha=ll/2.0, fit_intercept=False, max_iter=500000))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVrRrgB_dUiE"
      },
      "source": [
        "Utilicen esto para ajustar los datos con *features* polinomiales de grado nueve. Utilicen el mismo par谩metro de regularizaci贸n que el anterior: 0.001."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ImCloi_dUiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "804215ac-fa2a-45a0-fc8e-301a044a60b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=9)),\n",
              "                ('regressor',\n",
              "                 Lasso(alpha=0.005, fit_intercept=False, max_iter=500000))])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "lasso_pipe = lasso(9, 0.01)\n",
        "lasso_pipe.fit(x_train, t_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywq4jVIJdUiJ"
      },
      "source": [
        "Grafiquen los resultados con la ayuda del c贸digo de abajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPNOc2E8dUiK"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(x_train, t_train, 'o', ms=10, mfc='None', label='Entrenamiento')\n",
        "ax.plot(x_, lasso_pipe.predict(x_), 'r-', lw=3, alpha=0.8, label='Curva predicha')\n",
        "ax.plot(x_, ground_truth(x_), 'k-', lw=3, alpha=0.5, label='Modelo real')\n",
        "    #\n",
        "ax.set_title('Grado: {}; $\\lambda$: {:.2e}'.format(lasso_pipe['poly_features'].degree,\n",
        "                                                    lasso_pipe['regressor'].alpha *2), fontsize=16)\n",
        "    #\n",
        "ax.set_ylim(0, 3.9)\n",
        "ax.legend(loc=0, fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tipv4t4PdUiK"
      },
      "source": [
        "A primera vista, parece que ambos regresores producen los mismos resultados. Pero en realidad hay una _gran diferencia_ entre ambos m茅todos.\n",
        "\n",
        "**Compare los coeficientes encontrados con cada m茅todo**. Puede acceder a ellos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observaci贸n: para obtener los coeficientes de la rgreci贸n ridge, van a tener que replicar aqu铆 la parte de rige, como est谩 en el notebook de la clase."
      ],
      "metadata": {
        "id": "WHllSVDl2yYa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbVpnlkFdUiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e05dfe-28da-4e85-8250-656c150910b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.84607034,  0.19937739, -0.25641525, -0.2297371 , -0.07640904,\n",
              "         0.08150002,  0.21069503,  0.30738226,  0.37624621,  0.42342855]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "ridge_pipe['regressor'].coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBbnVQx2dUiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9496e892-27eb-47af-d661-a7b0e1117ddf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.08420373,  0.        , -0.        , -1.30247473, -0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  2.34130064])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "lasso_pipe['regressor'].coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ7DjLtqdUiL"
      },
      "source": [
        "**Pregunta**. 驴Ven alguna diferencia?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRuhlKPKdUiL"
      },
      "source": [
        "### Shrinkage recargado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pxTXWtZdUiL"
      },
      "source": [
        "Volvamos a hacer la gr谩fica de *shrinkage*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUSe9QrGdUiL"
      },
      "outputs": [],
      "source": [
        "# crea los valores de lambda\n",
        "lls = np.logspace(-4, 1, 100)\n",
        "\n",
        "cc_lasso = []\n",
        "\n",
        "# Itera sobre los valors de lambda, ajusta y guarda los valores de los coeficientes\n",
        "for ll in lls:\n",
        "    #print(ll)\n",
        "    lasso_pipe = lasso(degrees[-1], ll)\n",
        "    lasso_pipe.fit(x_train, t_train)\n",
        "    cc_lasso.append(lasso_pipe['regressor'].coef_)\n",
        "\n",
        "cc_lasso = np.array(cc_lasso)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAlRcFJVdUiM"
      },
      "source": [
        "Grafiquen las amplitudes de los coeficientes en funci贸n del par谩metro de regularizaci贸n para el caso de Lasso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSyMjVevdUiM"
      },
      "outputs": [],
      "source": [
        "# Valores de los coeficintes versus el par谩metro de renormalizaci贸n.\n",
        "fig = plt.figure(figsize=(8,7))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "for i in range(len(cc_lasso[0])):\n",
        "    ax.semilogx(lls, cc_lasso[:, i], label='$\\omega_{{{}}}$'.format(i), lw=3, alpha=0.6)\n",
        "ax.legend(ncol=3, fontsize=16)\n",
        "ax.set_xlabel('$\\lambda$')\n",
        "ax.set_ylabel('Valor del par谩metro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD5tSpqDdUiM"
      },
      "source": [
        "Como pueden ver, a medida que aumentamos el t茅rmino de regularizaci贸n, algunos par谩metros se van estrictamente a cero. De este modo, la regresi贸n Lasso tambi茅n funciona como una especie de herramienta de selecci贸n autom谩tica de modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1MXaZ_ndUiO"
      },
      "source": [
        "# Ejercicio avanzado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GOgB2nAdUiO"
      },
      "source": [
        "Hasta ahora, hemos elegido un polinomio de grado nueve y un valor fijo para el par谩metro de regularizaci贸n. Pero, 驴c贸mo sabemos que estos valores son los 贸ptimos?\n",
        "\n",
        "Al igual que hicimos con el regresor polin贸mico no regularizado, podemos utilizar el conjunto de pruebas para evaluar cu谩l es el valor 贸ptimo del grado *M* y del par谩metro de regularizaci贸n $\\lambda$.\n",
        "\n",
        "* Elija uno de los dos m茅todos de regularizaci贸n que hemos discutido anteriormente.\n",
        "* Haga un bucle sobre los valores de grado y lambda y eval煤e el modelo utilizando el conjunto de pruebas.\n",
        "* Elegir los mejores valores de $M$ y $\\lambda$ en base a estos resultados.\n",
        "* Repita para el otro regularizador.\n",
        "* Informe de la mejor m茅trica de rendimiento (MSE) en el conjunto de prueba.\n",
        "* Prepare un gr谩fico que muestre el MSE de la prueba en funci贸n de los valores de los hiperpar谩metros. Dado que hay dos hiperpar谩metros, quiz谩s quieras probar con `plt.imshow` o `plt.pcolor`.\n",
        "\n",
        "**Pregunta importante**: 驴piensan que ese valor del error cuadr谩tico medio es representativo de lo que obtendr铆a para datos que a煤n no se han visto en este proceso? 驴Que podr铆a pasar en ese caso?\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ka01-Nz9eKS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}