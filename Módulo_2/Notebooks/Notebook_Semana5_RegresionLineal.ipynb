{"cells":[{"cell_type":"markdown","metadata":{"id":"5HOQTYNi8Igw"},"source":["# Modelos lineales de regresión"]},{"cell_type":"markdown","metadata":{"id":"E_Jcrv918Ig1"},"source":["## Introducción"]},{"cell_type":"markdown","metadata":{"id":"tMTJqUcYUkaq"},"source":["Uno de los pasos más avanzados del proceso CRISP-DM es el **modelado**. ¿Qué significa exactamente esto?\n","\n","Los modelos son expresiones matemáticas más o menos simples (depende de la **familia de modelos** que se elija utilizar), que describen algún aspecto de los datos. Hoy vamos a charlar de la familia más conocida y sencilla de modelos, pero que es a la vez muy poderosa y versatil: los **modelos lineales de regresión**."]},{"cell_type":"markdown","metadata":{"id":"SP0s0Jtt8Ig1"},"source":["Los modelos lineales se encuentran entre los modelos más simples que podemos imaginar, pero siguen siendo extremadamente comunes y útiles. Tienen algunas propiedades analíticas simples y son extremadamente fáciles de entrenar e interpretar. Además, en la versión múltiple, son poderosos y versátiles."]},{"cell_type":"markdown","metadata":{"id":"sbBxi6nsUkar"},"source":["Un **modelo de regresión** es aquel en el que se busca describir el comportamiento de una variable (o un conjunto de variables) contínua a partir de otras variables del dataset. La variable que se busca describir (o *predecir*) se suele llamar variable objetivo (*target*). Las variables a partir de las cuales se busca hacer esto se llaman *covariables*, o *variables predictora*, o *variables independientes*, *regresores*, *features* (*características*), etc. Como muchas cosas importantes, tiene muchos nombres."]},{"cell_type":"markdown","metadata":{"id":"PaMup-juUkar"},"source":["Ejemplos de problemas de regresión:\n","\n","1. Predecir la mediana del precio de las casas en un distrito de California a partir de la mediana del ingreso en ese distrito y otras características del dataset.\n","2. Predecir la altura de los árboles en CABA a partir de su ancho y de la especie a la que pertenecen\n","3. Predecir el precio de un diamante a partir de sus quilates y el tipo de corte.\n","4. ..."]},{"cell_type":"markdown","metadata":{"id":"7YEZu4fk8Ig3"},"source":["### Celdas preparatorias"]},{"cell_type":"markdown","metadata":{"id":"CUokyrE88Ig4"},"source":["Como de costumbre, tenemos que arrancar importando distintos paquetes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8dM4NS-8Ig4"},"outputs":[],"source":["# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# Pandas\n","import pandas as pd\n","\n","# Common imports\n","import numpy as np\n","import os\n","from scipy import stats as st\n","\n","# Para hacer gráficos lindos\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)"]},{"cell_type":"markdown","metadata":{"id":"FA9wK7Dp8Ig4"},"source":["## Obtención y preparación de los datos."]},{"cell_type":"markdown","metadata":{"id":"u6tj4x37Uka9"},"source":["Vamos a suponer que tenemos interés en entender cómo depende el valor de venta de casas en la Ciudad de Buenos Aires de su superficie.\n","\n","Vamos a trabajar con un conjunto de datos de un portal inmobiliario Properati, que hasta hace un tiempo era público. Esta versión del dataset viene de [Kaggle](https://www.kaggle.com/datasets/paulrohan2020/latin-america-properties-published-in-properati), y lo modificamos un poco para hacerlo más simple.\n","\n","El dataset es muy grande, así que vamos a hacer algo de limpieza y filtrado antes de pasar al modelado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8KOTqutUka-","colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"status":"error","timestamp":1686580001256,"user_tz":180,"elapsed":461,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"b0c9a621-6dc6-4a93-f2d0-86904e96a3b2"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-f2ed2b302bcf>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/rodrigo/Documents/Teaching/ICD/Rcode/datasets'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'properati_arg_SPA.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/rodrigo/Documents/Teaching/ICD/Rcode/datasets/properati_arg_SPA.csv'"]}],"source":["DATA_DIR = '/Users/rodrigo/Documents/Teaching/ICD/Rcode/datasets'\n","df = pd.read_csv(os.path.join(DATA_DIR, 'properati_arg_SPA.csv'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgmAqScmUka-"},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"f5G3ar3lUka-"},"source":["En primer lugar vamos a quedarnos solo con los registros en los que las variables que nos interesan tienen valores. Para eso, usamos el método `dropna`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LY0BuqbnUka_"},"outputs":[],"source":["df = df.dropna(subset=['precio', 'sup_total'])\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"tdL94PiqUka_"},"source":["Quitemos algunas columnas con muy pocos datos, como `l5` y `l6`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8hFQ7ieUka_"},"outputs":[],"source":["# Con inplace=True, la variable df se sobreescribe\n","df.drop(['l5', 'l6'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"4i0wLoYWUka_"},"source":["Ahora algunos filtros:\n","\n","1. nos quedamos solo con las propiedades en la Ciudad de Buenos Aires.\n","\n","2. como la relación entre el precio y el tamaño puede cambiar significativamente de barrio en barrio, solo nos quedaremos con un barrio a elección (¡completen cada uno el valor del barrio!)\n","3. para tener un objeto más uniforme de estudio, solo utilizaremos las **ventas** de **casas**.\n","\n","Como queremos que se cumplan todas las condiciones, las concatenamos con `&`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdkWUEjdUkbA"},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpZ5_axxUkbB"},"outputs":[],"source":["barrio = 'Coghlan'\n","\n","df_filtro = df[(df.l2=='Capital Federal') &\n","               (df.l3 == barrio) &\n","               (df.tipo_propiedad == 'Casa') &\n","               (df.tipo_operac == 'Venta')]\n","\n","df_filtro.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6a2PDJuHUkbB"},"outputs":[],"source":["_ = pd.plotting.scatter_matrix(df_filtro, figsize=(12, 12))"]},{"cell_type":"markdown","metadata":{"id":"XiU16hMbUkbB"},"source":["Vemos un par de outliers adicionales, en (lat, lon) y en sup_cubierta"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFXaSGTHUkbC"},"outputs":[],"source":["df_filtro = df_filtro[(df_filtro.lon < -58.45) &\n","                      (df_filtro.sup_cubierta < 10000)]\n","\n","_ = pd.plotting.scatter_matrix(df_filtro, figsize=(12, 12))"]},{"cell_type":"markdown","metadata":{"id":"9eNzFC0aUkbC"},"source":["Perfecto! Ahora tenemos un dataset mucho más manejable, ideal para mostrar el funcionamiento de los modelos lineales.\n","\n","PEro antes de arrancar, veamos que los precios de la casas esten expresados de manera consistente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJTGn_BUUkbC"},"outputs":[],"source":["# Pedimos los valores únicos de la variable \"moneda\"\n","print(pd.unique(df_filtro.moneda))"]},{"cell_type":"markdown","metadata":{"id":"6P9n8LfYUkbD"},"source":["Ah, vemos que hay algunas casas con precios en Pesos y otras en Dólares. Veamos cuántas de cada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPPB5Y12UkbD"},"outputs":[],"source":["pd.value_counts(df_filtro.moneda)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWBErBz4UkbF"},"outputs":[],"source":["# Veamos el registro de esta casa\n","df_filtro[df_filtro.moneda=='ARS']"]},{"cell_type":"markdown","metadata":{"id":"ZJmI0uoBUkbG"},"source":["Pareciera que se trata de un error de ingreso de los datos, y que la moneda debería ser `USD`. Podemos hacer el cambio a mano, solo para ser prolijos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kegn1E3bUkbG"},"outputs":[],"source":["# Cambiamos a mano el valor de la variable moneda para este registro\n","# Atención, hay que usar .loc\n","df_filtro.loc[df_filtro.moneda=='ARS', 'moneda'] = 'USD'\n","\n","# Verificamos que ahora solo hay valores en dólares\n","df_filtro.value_counts('moneda')"]},{"cell_type":"markdown","metadata":{"id":"rH5AJBPKUkbG"},"source":["Ahora calculemos el coeficiente de Correlación de Pearson entre la variable target y el resto de las variables numéricas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSDm_v-RUkbH"},"outputs":[],"source":["df_filtro.corr(numeric_only=True).precio.sort_values()"]},{"cell_type":"markdown","metadata":{"id":"Fj9vZy-kUkbH"},"source":["Vemos que existe una importante correlación entre el precio de venta y la superficie total (como era esperable!)."]},{"cell_type":"markdown","metadata":{"id":"vu1Lw6_yUkbH"},"source":["### Visualización"]},{"cell_type":"markdown","metadata":{"id":"TMPUoSHiUkbI"},"source":["Hagamos una mínima visualización de lo que tenemos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3Cv4TzJUkbI"},"outputs":[],"source":["df_filtro.plot(kind='scatter', x='sup_total', y='precio', c='black',\n","               ylabel='Precio [en USD]')"]},{"cell_type":"markdown","metadata":{"id":"-LMRLpQ5UkbI"},"source":["Ahora sí estamos listos para arrancar."]},{"cell_type":"markdown","metadata":{"id":"5aYHVlvCUkbI"},"source":["## El rol de los modelos y las familias de modelos"]},{"cell_type":"markdown","metadata":{"id":"9RRcwJISUkbI"},"source":["El modelado de datos tiene varios objetivos:\n","\n","1. **Cuantificar** una relación (como la que vimos arriba). Es decir, ponerle números a eso: \"el precio de las casas es de X USD por metro cuadrado\")\n","2. **Explorar** los datos. Muchas veces, necesitamos quitar los patrones más obvios para poder detectar cosas más sutiles. En este caso, el patrón obvio es la dependencia con la superficie. ¿Habrá algún efecto secundario con, por ejemplo, el estado de la casa, o la cantidad de baños?\n","3. **Resumir** la información para transmitirla mejor. Un excelente complemento al gráfico de arriba es dar los números de los parámetros (ver abajo), que resultan una descripción compacta de los datos.\n","4. **Predecir** el valor de la variable *target* para una propiedad que no hemos observado."]},{"cell_type":"markdown","metadata":{"id":"o-7KW19EUkbQ"},"source":["## Regresión lineal simple"]},{"cell_type":"markdown","metadata":{"id":"0JFSDKexUkbR"},"source":["Como se dijo arriba, para modelar los datos de arriba, podemos elegir una familia de modelos. Acá vmaos a elegir a los modelos lineales (y por ahora simples; es decir con una única variable predictora)."]},{"cell_type":"markdown","metadata":{"id":"5GXpv0ClUkbR"},"source":["El modelo de regresión lineal más sencillo relaciona una variable *target* (en este caso el precio de las casas) con la covariable (en este caso, la superficie total), *x_1*, utilizando esta fórmula:\n","\n","$$\n","\\text{precio [USD]} = \\omega_0 + \\omega_1 \\cdot \\text{sup. total}\\;\\;,\n","$$\n","\n","donde $\\boldsymbol{\\omega} = (\\omega_0, \\omega_1)$ es el **vector de parámetros del modelo**.\n","\n","Esta fórmula define una **familia de modelos**. Todos los modelos de esta familia se ven como rectas en el gráfico de arriba, pero dependiendo del valor del vector de parámetros, pueden verse muy diferentes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FuEwku8tUkbS","colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"status":"error","timestamp":1686580784499,"user_tz":180,"elapsed":309,"user":{"displayName":"Martin Makler","userId":"14686016504460456268"}},"outputId":"578b76c0-1ab4-4d2b-f1c7-bf9703c009f5"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-4ba5b438c20a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df_filtro.plot(kind='scatter', x='sup_total', y='precio', c='black',\n\u001b[0m\u001b[1;32m      2\u001b[0m                ylabel='Precio [en USD]')\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Defino una serie de valores de los parámetros al azar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20230605\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_filtro' is not defined"]}],"source":["df_filtro.plot(kind='scatter', x='sup_total', y='precio', c='black',\n","               ylabel='Precio [en USD]')\n","\n","# Defino una serie de valores de los parámetros al azar\n","np.random.seed(20230605)\n","w0 = st.uniform(loc=-5e5, scale=2e6).rvs(100)\n","w1 = st.uniform(loc=-3000, scale=6000).rvs(100)\n","\n","# Creo un arreglo con dos valores (suficiente para una recta, entre el máximo y el mínimo de los valors de sup_total)\n","x = np.linspace(df_filtro.sup_total.min(), df_filtro.sup_total.max(), 2)\n","\n","# Para cada par de parámetros w0 y w1, grafico una recta\n","for ww0, ww1 in zip(w0, w1):\n","    plt.plot(x, ww0 + ww1 * x, '-', color='0.6', lw=0.5, alpha=0.5)"]},{"cell_type":"markdown","metadata":{"id":"NOAJnNiiUkbS"},"source":["De todas estas rectas, muy pocas tienen una similitud con los datos. Sin embargo, todas provienen de la misma familia de funciones.\n","\n","Para encontrar una recta que describa correctamente los datos, entonces tenemos que encontrar el valor adecuado de los parámetros. La buena noticia es que existe una manera analítica (es decir, exacta) de encontrar el valor de los parámetros que hacen que la curva se ajuste lo mejor posible a los datos. Pero antes de llegar a eso....\n","\n","**Pensemos**\n","\n","* ¿Qué características nos gustaría que tuviera la curva que describa a los datos?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z20GF_fXUkbS"},"source":["## Los residuos"]},{"cell_type":"markdown","metadata":{"id":"ftv7ELBwUkbS"},"source":["Para una recta cualquiera, podemos graficar la distancia entre los puntos y la recta como segmentos.\n","Como vamos a usar mucho el gráfico de los datos, definimos la función `plot_data`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nyg13tSsUkbT"},"outputs":[],"source":["def plot_data(data, xvar='sup_total', yvar='precio', **kwargs):\n","    color = kwargs.pop('c', 'black')\n","    mietiqueta = kwargs.pop('ylabel', 'Precio [en USD]')\n","    data.plot(kind='scatter', x=xvar, y=yvar, c=color,\n","                   ylabel=mietiqueta)\n","\n","plot_data(df_filtro)\n","\n","# Genero un número al azar entre 0 y 100\n","irandom = np.random.randint(100)\n","\n","# Grafico la curva correspondiente a ese número al azar\n","plt.plot(x, w0[irandom] + w1[irandom] * x, '-', color='0.6', lw=0.5)\n","\n","# Agrego los residuos\n","pred = w0[irandom] + w1[irandom] * df_filtro.sup_total\n","\n","# Grafico líneas verticales\n","plt.vlines(x=df_filtro.sup_total.values,\n","            ymin=df_filtro.precio.values,\n","            ymax=pred.values, color='0.5', lw=0.5)\n","\n","# Agrego los puntos sobre la recta\n","plt.plot(df_filtro.sup_total, pred, marker='o', color='red', mfc='None', ms=6)\n"]},{"cell_type":"markdown","metadata":{"id":"8D6ltT13UkbT"},"source":["A estas distancias, se las llama **residuos** de la recta. Pueden ser positivos, si los puntos negros están por encima de la recta, o negativos, si están por debajo.\n","\n","Matemáticamente, para el punto $i$, podemos escribir que el residuo $r_i$ es:\n","\n","$r_i = \\text{precio}_i - (\\omega_0 + \\omega_1 \\cdot \\text{sup. total}_i)\\;\\; .$\n","\n","Con esta definición, podemos aventurar:\n","\n","> El mejor conjunto de parámetros será el que haga que los residuos sean lo más chicos posibles.\n","\n","Pero está claro que como los residuos pueden ser negativos, podemos hacer que los residuos se achiquen todo lo que querramos mandando la recta para arriba, pero eso no tiene sentido. Entonces, reformulamos:\n","\n","> El mejor conjunto de parámetros será el que haga que **los valores absolutos** de los residuos sean lo más chicos posibles.\n","\n","Bien. Pero ahora se plantea la pregunta. ¿Los residuos de qué puntos? Fíjense que si agarro dos puntos cualquiera, con una recta puedo pasar exactamente por ellos y lograr que los residuos de esos puntos sean exactamente cero. Pero seguro que eso no es lo que queríamos decir, no?\n","\n","La idea es que sea lo más chico posible \"para todos\" los puntos... Lo más parecido que podemos hacer a esto es:\n","\n","> El mejor conjunto de parámetros será el que haga que **el promedio de los valores absolutos** de los residuos sean lo más chicos posibles.\n","\n","Ahora sí, llegamos a la definición del **error absoluto promedio (o medio)**, MAE, por sus siglas en inglés, que matemáticamente se escribe:\n","\n","$$\n","\\text{MAE} = \\frac{1}{N} \\left(|r_1| + |r_2| + \\cdots + |r_N|\\right) = \\frac{1}{N} \\sum_{i=1}^N \\left| r_i \\right |\\;\\;.\n","$$\n","\n","Por razones que escapan un poco el enfoque del curso, muchas veces se buscan los parámetros que minimizan el promedio de los residuos al cuadrado. Como el cuadrado es una función creciente, cuando se minimice uno, se minimiza el otro. Esto define el **error cuadrático promedio**, MSE:\n","\n","$$\n","\\text{MSE} = \\frac{1}{N} \\left(r_1^2 + r_2^2 + \\cdots + r_N^2\\right) = \\frac{1}{N} \\sum_{i=1}^N \\left( r_i \\right )^2\\;\\;.\n","$$\n","\n","Como se dijo arriba, bajo una serie de suposiciones, existe una manera matemática exacta de encontrar los valores de los parámetros que minimizan estas funciones\n"]},{"cell_type":"markdown","metadata":{"id":"3RByZC0DUkbV"},"source":["## Uso de las funciones de `scikit-learn`"]},{"cell_type":"markdown","metadata":{"id":"Lq51sIqnUkbV"},"source":["Tomemos ahora un modelo de regresión lineal de `sklearn` y utilicémoslo para ajustar estos datos. Para simplificar, cambiaremos primero los nombres de las variables relevantes.\n","\n","Además, las variables predictoras tienen que ir en formato matricial: los datos en cada fila y las covariables en cada columna. Esa matriz es la **matriz de diseño**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JtJ23iY_UkbW"},"outputs":[],"source":["# Variables predictoras (en una matriz, por eso la linea al final dle nombre)\n","X_ = df_filtro.sup_total.values.reshape(-1, 1)\n","t = df_filtro.precio.values\n","\n","print(X_.shape, t.shape)"]},{"cell_type":"markdown","metadata":{"id":"UbOc7TAFUkbX"},"source":["Instanciemos el regresor lineal y ajustemos los datos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cICE82y0UkbX"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","\n","# Instanciemos el modelo (le damos fit_intercept=True, para que ajuste también omega0)\n","lr = LinearRegression(fit_intercept=True)\n","\n","# Ajustamos (como siempre, con el método fit)\n","lr.fit(X_, t)"]},{"cell_type":"markdown","metadata":{"id":"Wbs2Ot-tUkbY"},"source":["**Nota fuera de contexto.**\n","\n","Estamos resolviendo las ecuaciones normales, ni más ni menos.\n","\n","$$\n","\\begin{array}{lll}\n","\\hat{\\omega_1} &=& \\sum_{i=0}^N (x_i - \\bar{X}) (t_i - \\bar{T})\\left[\\sum_{i=0}^N (x_i - \\bar{X})^2\\right]^{-1}\\\\\n","\\hat{\\omega_0} &=& \\bar{T} - \\hat{\\omega_1}\\bar{X}\n","\\end{array}\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"yH7ZXTbiUkbY"},"source":["### Los parámetros"]},{"cell_type":"markdown","metadata":{"id":"Tb1oPvFCUkbY"},"source":["Podemos ver el valor de los parámetros encontrados. El parámetro que no acompaña a ninguna variable ($\\omega_0$) está en el atributo `intercept_`. El resto (en este caso solo $\\omega_1$) están en el atributo `coef_`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_fj9-hXUkbZ"},"outputs":[],"source":["print('omega_0 = {:.3f}\\n omega_1 = {:.3f}'.format(lr.intercept_, lr.coef_[0]))"]},{"cell_type":"markdown","metadata":{"id":"vV2TPWt4UkbZ"},"source":["**Interpretemos estos números...**"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"k_DcBgUbUkba"},"source":["## Evaluación del modelo."]},{"cell_type":"markdown","metadata":{"id":"XFbTk6-SUkba"},"source":["Ahora que el modelo está ajustado (¡decir \"entrenado\" es demasiado en este contexto!), vamos a analizar si las cosas han funcionado como se esperaba.\n","\n","En esta sección evaluaremos si el modelo funciona como se esperaba, y obtendremos algunas ideas sobre cómo mejorarlo."]},{"cell_type":"markdown","metadata":{"id":"x-BiYrr7Ukba"},"source":["***\n","Antes de empezar, vamos a representar el modelo obtenido junto con los datos.\n","\n","Para ello, necesitamos obtener las predicciones que hace el modelo en una serie de puntos. Reucerden que para calcular las predicciones, todos los estimadores en `sklearn` usan el método `predict`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QFyaiEOUkba"},"outputs":[],"source":["# Plot de los datos\n","plot_data(df_filtro)\n","\n","# Calculamos la recta como antes, pero usando los parámetros del ajuste\n","y = lr.intercept_ + lr.coef_[0] * x\n","\n","# Otra forma es con el método predict\n","y = lr.predict(x.reshape(-1, 1))\n","\n","# Graficamos\n","plt.gcf().axes[0].plot(x, y, 'r-', lw=1)\n","\n","\n","## Agregamos residuos\n","\n","# predicciones del modelo en los puntos conocidos\n","pred = lr.predict(X_)\n","\n","# Grafico líneas verticales\n","plt.vlines(x=df_filtro.sup_total.values,\n","            ymin=df_filtro.precio.values,\n","            ymax=pred, color='0.5', lw=0.5)"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"cPBN76AVUkbb"},"source":["#### Métricas"]},{"cell_type":"markdown","metadata":{"id":"Z5gLraAZUkbb"},"source":["##### Error cuadrático medio"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"04z52c85Ukbb"},"source":["La primera forma de evaluar un modelo es calcular la métrica que se utilizaron para ajustar los parámetros del modelo, la MAE y MSE.\n","\n","Ambas están implementadas en `sklearn`, en el paquete `metrics`, y se usan de manera idéntica: se les pase el valor de la variable target y lo que el modelo predice para esos datos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aoSk9BWUkbb"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","print('MAE [K$] = {:.2f}'.format(mean_absolute_error(t, pred)/1e3))\n","print('MSE [$^2]= {:.2f}'.format(mean_squared_error(t, pred)))"]},{"cell_type":"markdown","metadata":{"id":"MKatSgNZUkbc"},"source":["Como las unidades del MSE son dólares al cuadrado, muchas\n","veces se reporta la raíz del error cuadrático medio, RMSE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnPLWfMcUkbc"},"outputs":[],"source":["print('RMSE [K$]= {:.2f}'.format(np.sqrt(mean_squared_error(t, pred))/1e3))"]},{"cell_type":"markdown","metadata":{"id":"GuOzL6cfUkbc"},"source":["Vemos que las métricas no dan el mismo número, pero sí estamos seguros de que ambas son mínimas."]},{"cell_type":"markdown","metadata":{"id":"AMBFNItMUkbd"},"source":["##### Coeficiente de determinación"]},{"cell_type":"markdown","metadata":{"id":"iKHC7-faUkbd"},"source":["Otra métrica muy frecuentemente usada para evaluar un modelo es el coeficiente de determinación:\n","\n","$$\n","R^2 = \\frac{SC_\\mathrm{tot} - SC_\\mathrm{res}}{SC_\\mathrm{tot}}\\;\\;,\n","$$\n","\n","que refleja la parte de la varianza de los datos que el modelo alcanza a explicar. Esto se puede ver de la expresión (no obvia):\n","$$\n","\\underbrace{\\sum_{i=1}^N\\left(t - \\bar{t}\\right)^2}_{SC_\\mathrm{tot}} = \\underbrace{\\sum_{i=1}^N\\left(t - y\\right)^2}_{SC_\\mathrm{res}} + \\underbrace{\\sum_{i=1}^N\\left(y - \\bar{t}\\right)^2}_{SC_\\mathrm{reg}}\\;\\;.\n","$$\n","\n","Se puede ver que $R^2$ toma valores entre cero y uno. El paquete `sklearn.metrics` tiene una implementación de esta métrica: `r2_score`. Además, es la métrica por defecto de muchos algoritmos de regresión, bajo el método `score`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7n4BX8j8Ukbd"},"outputs":[],"source":["from sklearn.metrics import r2_score\n","print('R^2 (conjunto de entrenamiento) = {:.3f}'.format(r2_score(t, pred)))"]},{"cell_type":"markdown","metadata":{"id":"ti1VHmtvUkbd"},"source":["Esto significa que alrededor del 84\\% de la varianza de los datos desaparece con el modelo lineal simple.\n","\n","Una linda propiedad es que $R^2$ es el cuadrado del coeficiente de Pearson entre $X$ y $t$."]},{"cell_type":"markdown","metadata":{"id":"l8LIcjxAUkbe"},"source":["***\n","Por ahora, no estamos separando el conjunto de entrenamiento de un conjunto de testeo. Esto es tema de la segunda parte del módulo. Pero sepan que todos los valores de las métricas que conseguimos son en relidad optimistas.\n","***"]},{"cell_type":"markdown","metadata":{"id":"2LwUB0n1Ukbe"},"source":["#### Predicciones vs. Targets"]},{"cell_type":"markdown","metadata":{"id":"TngA1wTWUkbe"},"source":["Otra buena forma de evaluar el modelo es visualizando los datos y las predicciones.\n","\n","En el gráfico de abajo está la predicción del modelo (eje y) vs. el valor de la variable target (eje x)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a_GzM53wUkbe"},"outputs":[],"source":["# Graficar las predicciones contra los valores target.\n","fig = plt.figure(figsize=(8,6))\n","ax = fig.add_subplot(111)\n","ax.plot(t, pred, 'ok', mfc='None', ms=4, alpha=0.5)\n","\n","ax.set_xlabel('Valores target (precio de las casas)')\n","ax.set_ylabel('Predicciones del modelo')\n","\n","# Agregar la línea identidad\n","ax.plot([t.min(), t.max()], [t.min(), t.max()], color='r', lw=3)"]},{"cell_type":"markdown","metadata":{"id":"sql3HOxaUkbf"},"source":["**Pregunta.** ¿Creen que el modelo está funcionando correctamente? Si no, ¿qué podrían señalar como deficiencia?"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"IAYL4K-ZUkbf"},"source":["#### Residuos"]},{"cell_type":"markdown","metadata":{"id":"SSRhzTH_Ukbf"},"source":["También es fundamental explorar el comportamiento de los residuos del modelo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Gc_7kc_Ukbf"},"outputs":[],"source":["# Calculamos los residuos a partir de la variable target y las predicciones del modelo\n","res = t - pred"]},{"cell_type":"markdown","metadata":{"id":"G5PfChAYUkbg"},"source":["##### Residuos vs. Predictores"]},{"cell_type":"markdown","metadata":{"id":"JgKOCJnCUkbg"},"source":["Hay varios gráficos que son importantes.\n","\n","En primer lugar, ver los residuos en función de la variable predictora:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzU3SenzUkbg"},"outputs":[],"source":["fig = plt.figure(figsize=(8,6))\n","ax = fig.add_subplot(111)\n","ax.plot(X_, res/1e3, 'ok', mfc='None', ms=4, alpha=0.5)\n","\n","# Agragamos líneas vertical y horizontal\n","ax.axhline(0, color='r', ls=':')\n","ax.axvline(X_.mean(), color='r', ls=':')\n","\n","# Add axes labels\n","ax.set_xlabel('Superficie total de las casas [m2]')\n","ax.set_ylabel('Residuos del modelo [miles de USD]')"]},{"cell_type":"markdown","metadata":{"id":"fmG4FZJLUkbh"},"source":["Este gráfico es útil para identificar las curvaturas y otras características que podrían indicar que el modelo elegido o las variables predictoras son inadecuados."]},{"cell_type":"markdown","metadata":{"id":"7_XCh6NrUkbi"},"source":["***\n","**Nota fuera de contexto**\n","\n","Las ecuaciones normales implican dos cosas:\n","1. los residuos tienen media cero.\n","2. el coeficiente de Pearson entre la variable X y los residuos es cero.\n","\n","Vamos a verificar estas propiedades con el código de acá abajo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5-MMwGiUkbi"},"outputs":[],"source":["xmean = X_.mean()\n","resmean = res.mean()\n","print('$\\\\bar{{y - t}}$ = {:.12f}'.format(resmean))\n","print('$rho(X, res)$ = {:.12f}'.format(np.corrcoef(X_.flatten(), res)[0, 1]))"]},{"cell_type":"markdown","metadata":{"id":"-4SvRFLtUkbi"},"source":["***"]},{"cell_type":"markdown","metadata":{"id":"A5qYBZ9tUkbj"},"source":["##### Residuos vs. Valores predichos\n"]},{"cell_type":"markdown","metadata":{"id":"zZmO-vMxUkbj"},"source":["Un gráfico similar es el de los residuos en función de los valores predichos."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"mm-PEqGfUkbj"},"outputs":[],"source":["fig = plt.figure(figsize=(8,6))\n","ax = fig.add_subplot(111)\n","ax.plot(pred/1e6, res/1e3, 'ok', mfc='None', ms=4, alpha=0.5)\n","\n","ax.axhline(0, color='r', ls=':')\n","ax.set_xlabel('Valores predichos [millones de USD]')\n","ax.set_ylabel('Residuos del modelo [miles de USD]')\n"]},{"cell_type":"markdown","metadata":{"id":"8mNfY9GgUkbj"},"source":["Este gráfico es su amigo, porque puede construirse incluso en el caso de la regresión lineal múltiple.\n","\n","Al igual que arriba, este gráfico es útil para identificar las curvaturas."]},{"cell_type":"markdown","metadata":{"id":"Hpt07Y1kUkbj"},"source":["## TEMA AVANZADO: Palanca"]},{"cell_type":"markdown","metadata":{"id":"4bFHywFVUkbm"},"source":["La *palanca* (*leverage* en inglés) es un concepto interesante para evaluar ajustes a modelos lineales.\n","\n","Se puede mostrar que el valor predicho por el modelo para un dado valor $x_i$ puede expresarse como\n","\n","$$\n","y_i = \\hat{\\omega_0} + \\hat{\\omega_1} x_i = \\sum_{k=1}^N h_{ik} t_k\\;\\;,\n","$$\n","es decir como una combinación lineal de los valores de la variable *target*. La suma que aparece en el término de la derecha corre sobre todos los puntos del dataset. Es decir que para saber la predicción de un punto tenemos que sumar los valores de la variable *target* de todos los puntos $t_k$, pero ponderados por $h_{ik}$:\n","\n","$$\n","h_{ik} = \\frac{1}{N} + \\frac{\\left(x_i - \\bar{x}\\right)\\left(x_k - \\bar{x}\\right)}{S_{xx}}\\;\\;,\n","$$\n","\n","con\n","$$\n","\\bar{x} = \\frac{1}{N} \\sum_{i=1}^N x_i\n","$$\n","y\n","$$\n","S_{xx} = \\sum_{i=1}^N \\left(x_i - \\bar{x}\\right)^2\\;\\;.\n","$$\n","\n","El elemento $h_{ik}$ nos dice cuánto contribuye un dado punto a la predicción $y_i$.\n"]},{"cell_type":"markdown","metadata":{"id":"X6DCTjfTUkbn"},"source":["Vamos a calcular la matriz cuyos elementos $ij$ son $h_{ij}$. Esta matrix se llama matriz sombrero o sombrerera."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-uwk7dr2Ukbn"},"outputs":[],"source":["from scipy.linalg import cho_factor, cho_solve\n","\n","def hat_matrix(X, include_bias=True):\n","    \"\"\"\n","    Compute hat matrix for design matrix X.\n","\n","    :param np.array X: design matrix of dimensions (n x d),\n","    where n is the number of observations and d is the number of\n","    features.\n","    :param bool include_bias: if True (default), then include a bias column,\n","    in design matrix X (i.e. a column of ones - acts as an\n","    intercept term in a linear model).\n","    \"\"\"\n","    if include_bias:\n","        X = np.hstack([np.ones([len(X), 1]), X])\n","\n","    A = np.matmul(X.T, X)\n","\n","    LL = cho_factor(A)\n","    return np.matmul(X, cho_solve(LL, X.T))\n","\n","# Define HAT matrix, whose diagonal are the leverage values\n","h = hat_matrix(X_)"]},{"cell_type":"markdown","metadata":{"id":"_dstY9j4Ukbo"},"source":["**Pregunta.** ¿Cuál es la forma de la matriz sombrerera? Verifiquen con el atributo `shape`."]},{"cell_type":"markdown","metadata":{"id":"cr0H1E6RUkbp"},"source":["Esta matriz nos dice cómo influye cada punto en la predicción de todos los demás.\n","\n","En particular, podemos trazar la influencia de cada observación sobre sí misma. Esto esto es la palanca de la observación, y se obitene como la diagonal de la matrix H. Noten que esto es independiente de $t$; sólo se refiere a los valores de la variable predictora)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTbV5yAzUkbp"},"outputs":[],"source":["leverage = np.diag(h)\n","plt.figure(figsize=(10, 6))\n","plt.plot(X_.flatten(), leverage, '.')\n","plt.xlabel('Ingreso medio [estandarizado]')\n","plt.ylabel('Palanca')"]},{"cell_type":"markdown","metadata":{"id":"lt2tcIckUkbp"},"source":["##### Residuos vs. Palanca"]},{"cell_type":"markdown","metadata":{"id":"lKqklx07Ukbp"},"source":["Un gráfico muy informativo es el de los residuos como función de la palanca."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUmGfbxVUkbq"},"outputs":[],"source":["fig = plt.figure(figsize=(10,6))\n","ax = fig.add_subplot(111)\n","\n","ax.plot(leverage, res/1e3, 'ok', ms=4, mfc=None, alpha=0.2)\n","ax.axhline(0, color='r', ls=':')\n","ax.set_xlabel('Palanca [$h_{ii}$]')\n","ax.set_ylabel('Residuos del modelo [K$]')"]},{"cell_type":"markdown","metadata":{"id":"omQRZFWoUkbq"},"source":["En este gráfico, podemos ver que hay unos pocos puntos con una palanca elevada y residuos distintos de cero. Estos puntos suelen ser muy relevantes para el ajuste, ya que tiran del modelo hacia ellos con más fuerza que el resto. Como hay tantos puntos en total en este caso, no esperamos que estos valores influyan mucho en el ajuste final, pero en una situación con menos datos, estos datos pueden ser muy influyentes.\n","\n","Cuando los valores de palanca están muy pegados, podemos ver el gráfico en escala logarítimica."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUefJz0qUkbq"},"outputs":[],"source":["# Mismo gráfico en escala log\n","fig = plt.figure(figsize=(8,6))\n","ax = fig.add_subplot(111)\n","\n","ax.semilogx(np.diag(h), res/1e3, 'ok', ms=4, mfc=None, alpha=0.2)\n","ax.axhline(0, color='r', ls=':')\n","ax.set_xlabel('Palanca [$h_{ii}$]')\n","ax.set_ylabel('Residuos del modelo [K$]')"]},{"cell_type":"markdown","metadata":{"id":"bOA_ExP2Ukbq"},"source":["##### Otra propiedad de las palancas"]},{"cell_type":"markdown","metadata":{"id":"zrKMLqYYYMKf"},"source":["Se puede probar que la suma de las *palancas* son un valor constante:\n","\n","$$\n","\\sum_{i=1}^N h_{ii} = 2\\;\\;.\n","$$\n","\n","Por lo tanto, cada vez que un punto gana en palanca, el resto pierde. En general, la suma de todos los valores de *palanca* debe ser igual a las dimensiones del parámetro, $\\boldsymbol{\\omega}$.\n","\n","Verifiquemos la propiedad con el código"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfo7GN-qUkbr"},"outputs":[],"source":["print('La suma de las palancas es {:.2f}.'.format(np.sum(np.diag(h))))"]},{"cell_type":"markdown","metadata":{"id":"j72F_tGyUkbr"},"source":["### TEMA AVANZADO: Residuos estandarizados"]},{"cell_type":"markdown","metadata":{"id":"Ql5ieo2NUkbr"},"source":["En realidad, los gráficos de los residuos pueden ser engañosos, porque la varianza de los residuos no es constante (¡la varianza de los *errores* sí lo es!). Se puede demostrar que la varianza de los residuos es\n","\n","$$\n","\\text{var}(r_i) = \\sigma^2 (1 - h_{ii})\\;\\;,\n","$$\n","donde $h_{ii}$ es la palanca del punto $i$. Si no conocemos la dispersión del término de error, $\\sigma^2$, podemos usar el estimador:\n","\n","$$\n","\\widehat{\\sigma^2} = \\frac{1}{N-2} \\sum_{i=1}^N \\left(y_i - t_i\\right)^2\\;\\;.\n","$$\n","\n","Podemos entonces calcular los **residuos estandarizados** que tienen la misma varianza para todo el rango de la variable predictora $X$:\n","\n","$$\n","\\text{st-res}_i = \\frac{y_i - t_i}{\\sqrt{\\widehat{\\sigma^2} \\left(1 - h_{ii}\\right)}}\\;.\n","$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vVUCdMa1Ukbr"},"outputs":[],"source":["# Calculamos residuos estandarizados\n","stres = (t - pred) / np.sqrt(res.std(ddof=2) * (1 - leverage))"]},{"cell_type":"markdown","metadata":{"id":"ExxfgdwtUkbs"},"source":["\n","Y podemos usarlos para  hacer gráficos análogos a los de arriba."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W5POP3MFUkbs"},"outputs":[],"source":["fig = plt.figure(figsize=(8,6))\n","ax = fig.add_subplot(111)\n","\n","ax.semilogx(np.diag(h), stres/1e3, 'ok', ms=4, mfc=None, alpha=0.2)\n","ax.axhline(0, color='r', ls=':')\n","ax.set_xlabel('Palanca [$h_{ii}$]')\n","ax.set_ylabel('Residuos estandarizados [K$]')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hu2mUj4SUkbs"},"outputs":[],"source":["fig = plt.figure(figsize=(8,6))\n","ax = fig.add_subplot(111)\n","ax.plot(pred, stres, 'ok', mfc='None', ms=4, alpha=0.5)\n","\n","ax.axhline(0, color='r', ls=':')\n","ax.set_xlabel('Valor predicho')\n","ax.set_ylabel('Residuos estandarizados')"]},{"cell_type":"markdown","metadata":{"id":"IY1Un67WUkbs"},"source":["## Errores en los parámetros"]},{"cell_type":"markdown","metadata":{"id":"hH3obARNUkbt"},"source":["Los valores de los parámetros obtenidos dependen de las características particulares de este dataset.\n","\n","Piensen qué pasaría si tuviéramos acceso a un dataset similar, de propiedades del mismo tipo, en el mismo barrio, pero de otra inmobiliaria. Seguramente si repetimos el proceso encontraríamos parámetros similares, pero no exactamente igual.\n","\n","La idea de que estos parámetros entonces tienen un cierto \"margen de error\" es clave. ¿Pero como no tenemos acceso a otros datasets, cómo podemos estimar el error de nuestros parámetros?"]},{"cell_type":"markdown","metadata":{"id":"gIbWfUHHUkbt"},"source":["Una opción es repetir el ajuste cambiando ligeramente el conjunto de datos que tenemos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MC2J-xleUkbt"},"outputs":[],"source":["plot_data(df_filtro)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rxM8VbDGUkbt"},"outputs":[],"source":["# Repetir el experimento varias veces\n","Nrepet = 10000\n","\n","# Creo listas vacias para ir llenando\n","w0_lista = np.zeros(Nrepet)\n","w1_lista = np.zeros(Nrepet)\n","\n","for i in range(Nrepet):\n","    # Elijo al azar 53 números (puede haber repetidos)\n","    irandom = np.random.randint(0, len(df_filtro)-1, size=len(df_filtro))\n","\n","    # Construyo un nuevo set de datos a partir de estos números\n","    X_new = X_[irandom]\n","    t_new = t[irandom]\n","\n","    # Ajusto el modelo nuevamente con estos datos\n","    lr.fit(X_new, t_new)\n","\n","    # Meto el resultado del ajuste en la lista\n","    w0_lista[i] = lr.intercept_\n","    w1_lista[i] = lr.coef_[0]\n",""]},{"cell_type":"markdown","metadata":{"id":"Jh8S1_oKUkbu"},"source":["Con las listas de parámetros, podemos calcular algunas métricas.\n","Para eso, nada como usar `pandas`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-5qOLJ-Ukbu"},"outputs":[],"source":["parametros = pd.DataFrame(data={'w0': w0_lista, 'w1': w1_lista})\n","parametros.describe()"]},{"cell_type":"markdown","metadata":{"id":"jqXJypQMUkbu"},"source":["Podemos también graficar los resultados como histogramas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYXGtV5pUkbu"},"outputs":[],"source":["parametros.hist(bins=50)"]},{"cell_type":"markdown","metadata":{"id":"XqHGEM1lUkbv"},"source":["### Significancia"]},{"cell_type":"markdown","metadata":{"id":"bVwfE_KTUkbv"},"source":["¿Qué creen que pasaría si repetimos todo el procedimiento de arriba pero usando una variable que no es relevante pare el análisis?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zAP2E5lMUkbv"},"outputs":[],"source":["plot_data(df_filtro, xvar='lat')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Squg7Fz5Ukbv"},"outputs":[],"source":["X_lat = df_filtro.lat.values.reshape(-1, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyXI1hMlUkbv"},"outputs":[],"source":["# Repetir el experimento varias veces\n","Nrepet = 10000\n","\n","# Creo listas vacias para ir llenando\n","w0_lista = np.zeros(Nrepet)\n","w1_lista = np.zeros(Nrepet)\n","\n","for i in range(Nrepet):\n","    # Elijo al azar 53 números (puede haber repetidos)\n","    irandom = np.random.randint(0, len(df_filtro)-1, size=len(df_filtro))\n","\n","    # Construyo un nuevo set de datos a partir de estos números\n","    X_new = X_lat[irandom]\n","    t_new = t[irandom]\n","\n","    # Ajusto el modelo nuevamente con estos datos\n","    lr.fit(X_new, t_new)\n","\n","    # Meto el resultado del ajuste en la lista\n","    w0_lista[i] = lr.intercept_\n","    w1_lista[i] = lr.coef_[0]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5hEfRCFUkbw"},"outputs":[],"source":["parametros_lat = pd.DataFrame(data={'w0': w0_lista, 'w1': w1_lista})\n","parametros_lat.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l23yVrGgUkbw"},"outputs":[],"source":["parametros_lat.hist(bins=50)"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"X8S6nxcAUkbw"},"source":["# Su turno. Modelo lineal múltiple"]},{"cell_type":"markdown","metadata":{"id":"odbI3tfJUkbw"},"source":["\n","***\n","* Hagan un análisis similar eligiendo una variable predictiva diferente.\n","\n","* Compare el rendimiento del modelo lineal simple que utiliza \"sup_total\" con el que han elegido.\n","\n","* Ahora viene la parte bonita. Incluya en la matriz X una nueva columna, de modo que combine la \"sup_total\" con su nueva variable, y realice un análisis similar para este \"modelo lineal múltiple\". ¿Qué gráficos puedes hacer todavía? **Tip**: tal vez se pueda crear una variable `sup_fondo`..."]},{"cell_type":"markdown","metadata":{"id":"CeLZEf00Ukbx"},"source":["**Tip**: Para incluir una nueva variable predictora en X (la matriz de diseño); cambien YOUR_FEATURE por el nombre de la columna que desean incluir."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0vJYSUeUkbx"},"outputs":[],"source":["X = df_filtro.loc[:, ['sup_total', 'YOUR_FEATURE1', 'YOUR_FEATURE2', 'YOUR_FEATURE3', ...]].values"]},{"cell_type":"markdown","metadata":{"id":"RPRmCCd8YMKp","tags":[]},"source":["# Extra! Un procedimiento para detectar outliers"]},{"cell_type":"markdown","metadata":{"id":"gYHTj8M-YMKp"},"source":["Es importante comprobar la presencia de valores atípicos (*outliers*). Si por alguna razón los datos tienen algún valor que no proviene del proceso real que estamos modelando, esto puede modificar los resultados del ajuste y, en particular, sesgarlo fuertemente.\n","\n","Por ejemplo, tomemos los datos sintéticos y hagamos que uno de los puntos se convierta en un valor atípico, fijando un valor a mano. Para que el efecto sea claro, tenemos que hacerlo en un punto que tenga una alta palanca. Veamos..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAaud6UtYMKp"},"outputs":[],"source":["def ground_truth(x):\n","    return 3*x + 4 #+ 0.1*x**2\n","\n","# Función que genera un dataset al azar en base a la \"ground truth\"\n","def make_default_dataset(real_process, sigma=0.5, high_leverage=None, random_seed=20210331):\n","    # Fijo seed\n","    np.random.seed(random_seed)\n","\n","    # Defino vector de x\n","    x = np.random.rand(20)\n","\n","    # Por si quiero otra nube de puntos\n","#     x2 = np.random.rand(4) + 2.5\n","#     x = np.concatenate([x, x2])\n","\n","    x = np.sort(x)\n","\n","    # Agrego un punto con mucha palanca\n","    if high_leverage is not None:\n","        high_leverage_x = np.array(high_leverage)\n","        x = np.append(x, high_leverage_x)\n","\n","    x_plot = np.linspace(x.min(), x.max(), 100).reshape(-1, 1)\n","\n","    # Error\n","    t = real_process(x) + np.random.randn(len(x)) * sigma\n","\n","    return x, t, x_plot\n","\n","x, t, x_plot = make_default_dataset(ground_truth, high_leverage=3)\n","\n","# Recalculo la palanca\n","h = np.diag(hat_matrix(x.reshape(-1, 1)))\n","\n","# Defino una observación outlier\n","t[-1] = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlr4_MJkYMKp","tags":[]},"outputs":[],"source":["plt.figure(figsize=(9, 6))\n","plt.plot(x, t, 'o', ms=10)\n","plt.plot(x_plot, ground_truth(x_plot), '-', color='0.5', label='Proceso real')\n","plt.xlabel('X', fontsize=16)\n","plt.ylabel('t', fontsize=16)\n","\n","# Ajuste y grafico\n","lr = LinearRegression()\n","lr.fit(x.reshape(-1, 1), t)\n","plt.plot(x_plot, lr.predict(x_plot), '-r', lw=4, alpha=0.5, label='Ajuste')\n","plt.legend(loc=0, fontsize=16)\n","\n","# Calculo residuos\n","res = t - lr.predict(x.reshape(-1, 1))"]},{"cell_type":"markdown","metadata":{"id":"TMWqsKN3YMKp"},"source":["Por supuesto, si no conociéramos el proceso real, identificar el punto en $x=3,0$ como *outlier* sería más difícil. Y recordemos que en muchas dimensiones todo es más difícil.\n","\n","Veamos el gráfico de residuos vs. la palanca, a ver si nos da más pistas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNc1xbUPYMKq"},"outputs":[],"source":["plt.figure(figsize=(9, 6))\n","plt.plot(h, res, 'o', ms=10)\n","plt.axhline(0, color='r', ls=':')\n","# plt.gca().set_xscale('log')\n","\n","plt.ylabel('Residuos', fontsize=16)\n","plt.xlabel('Palanca', fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"DyXPLWyKYMKq"},"source":["Aunque el punto atípico está bastante lejos de cero, no es dramáticamente extraño. Está claro lo que ocurre aquí: el *outlier* ya ha modificado la solución y, por tanto, el residuo es relativamente pequeño en ese punto.\n","\n","**¿Cómo podemos hacer entonces? ¿Ideas? ¿Ideas que funcionen en muchas dimensiones? **"]},{"cell_type":"markdown","metadata":{"id":"gf9xD4FXYMKq"},"source":["### Entra Validación Cruzada"]},{"cell_type":"markdown","metadata":{"id":"-Wi3Pb3vYMKq"},"source":["Una posibilidad es hacer uso de una herramienta que será fundamental en el curso para detectar y evitar el sobreajuste, que se llama validación cruzada.\n","\n","En este caso la vamos a utilizar para detectar el punto atípico. En cierto modo, también se trata de un sobreajuste, porque el modelo intenta captar todo el conjunto de entrenamiento, en lugar de preocuparse por reproducir la tendencia general, aquí dada por el proceso real (¡que por suerte conocemos en este caso!)."]},{"cell_type":"markdown","metadata":{"id":"4n7c0GT2YMKr"},"source":["La idea es construir subconjuntos de datos, partiendo del conjunto original, sacando un punto cada vez. Es decir, se construyen $N$ conjuntos de $N-1$ datos, y se ajusta un modelo para cada uno de esos nuevos datos. A continuación, se compara la predicción realizada por cada modelo para el punto que hemos eliminado con la realizada por el modelo ajustado a todos los datos.\n","\n","La intuición es que cuando eliminamos el valor atípico que está influyendo en el ajuste, la predicción cambiará drásticamente."]},{"cell_type":"markdown","metadata":{"id":"ywAVljx3YMKr"},"source":["Esto se llama *validación cruzada de uno a uno* (*leave-one-out cross-validation*), porque los puntos se sacan uno a uno. Hay muchos más sabores de validación cruzada, algunos de los cuales cubriremos más adelante en el curso."]},{"cell_type":"markdown","metadata":{"id":"8EvVdESAUkb0"},"source":["**Ejercicio**. Corran el código de la próxima celda e intenten entender qué hacen `LeaveOneOut` y `LeavePOut`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_9dn6GTYMKr"},"outputs":[],"source":["# Implementación de LOOCV en sklearn\n","from sklearn.model_selection import LeaveOneOut, LeavePOut, cross_val_predict\n","\n","loo = LeaveOneOut()\n","# loo = LeavePOut(3)\n","for train, test in loo.split(x):\n","    print(train, test)"]},{"cell_type":"markdown","metadata":{"id":"Q7DZeLFCUkb1"},"source":["A continuación, utilicen la función `cross_val_predict` para calcular la predicción en todos los puntos, pero sólo cuando no estén incluidos en el conjunto de entrenamiento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xkaSJCskYMKr"},"outputs":[],"source":["# Cálculo de las predicciones de la regresión lineal dejando cada punto afuera\n","y_iout = cross_val_predict(lr, x.reshape(-1, 1), t, cv=loo)"]},{"cell_type":"markdown","metadata":{"id":"h2z0qKjNUkb3"},"source":["Verificamos la forma de la salida"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGaKHzJNYMKr"},"outputs":[],"source":["print(y_iout.shape, x.shape)"]},{"cell_type":"markdown","metadata":{"id":"ZR2Y_LxVUkb3"},"source":["Por si no se entendió lo que acabamos de hacer, acá hay un código que lo hace a mano."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMFm0I-0YMKs"},"outputs":[],"source":["y_iout_mano = np.empty_like(t)\n","\n","for i, [train, test] in enumerate(loo.split(x)):\n","    x_i = x[train]\n","    t_i = t[train]\n","\n","    lr.fit(x_i.reshape(-1, 1), t_i)\n","    y_ii = lr.predict(x[test].reshape(-1, 1))\n","\n","    y_iout_mano[i] = y_ii"]},{"cell_type":"markdown","metadata":{"id":"72qkdOb7Ukb4"},"source":["Y podemos verificar que dan lo mismo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Z_l625gYMKs"},"outputs":[],"source":["np.allclose(y_iout, y_iout_mano)"]},{"cell_type":"markdown","metadata":{"id":"QGn35lX9Ukb_"},"source":["Por último graficamos la diferencia de las predicciónes para cada punto."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIaaDl1tYMKs"},"outputs":[],"source":["lr.fit(x.reshape(-1, 1), t)\n","y = lr.predict(x.reshape(-1, 1))\n","\n","plt.plot(x, (y - y_iout), 'or', ms=10, mfc='None')\n","plt.xlabel('X', fontsize=16)\n","plt.ylabel('$Y_i - Y_{i(i)}$')"]},{"cell_type":"markdown","metadata":{"id":"4WfzNNsqYMKs"},"source":["Podemos ver que el valor atípico es claramente visible.\n","\n","Para casos menos obvios, podemos definir una estadística que se compare con alguna distribución razonable, pero no entraremos en esos detalles aquí."]},{"cell_type":"markdown","metadata":{"id":"q0eZj7diYMKt"},"source":["### Relevancia de una observación"]},{"cell_type":"markdown","metadata":{"id":"nPotND-fYMKt"},"source":["Obviamente, un valor atípico como el que acabamos de ver es una observación muy influyente, que determina fuertemente el resultado del ajuste/entrenamiento.\n","\n","Pero si ese mismo valor atípico hubiera tenido menos influencia, las cosas habrían sido diferentes."]},{"cell_type":"markdown","metadata":{"id":"KCeCbsbcYMKt"},"source":["**Ejercicio**\n","\n","* Modificar la posición y el valor del *outlier*. Ver qué influencia tiene en cada caso.\n","* Discutir qué combinación tiene que darse para que una medida sea influyente.\n","***"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 ('tf-mac')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"nav_menu":{"height":"279px","width":"309px"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"288px"},"toc_section_display":"block","toc_window_display":true},"vscode":{"interpreter":{"hash":"2b7aa682480b82eb27ca7b5ecfebdb0027bb2a276e6bdff64c1ddeab03557e9e"}}},"nbformat":4,"nbformat_minor":0}